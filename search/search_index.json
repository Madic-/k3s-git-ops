{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"K3S","text":"<p>Warning</p> <p>This documentation is at a very early stage.</p>"},{"location":"#important-used-technologies","title":"Important used technologies","text":"<ul> <li> <p>K3S This is the foundation of the infrastructure. It's an easy to deploy and maintain kubernetes distribution</p> </li> <li> <p>ArgoCD is a tool to manage kubernetes clusters the GitOPs way</p> </li> <li> <p>Kustomize is used to manage the kubernetes manifests within ArgoCD</p> </li> <li> <p>Ansible prepares the machines for the k3s installation and installs k3s</p> </li> <li> <p>Vagrant manages the test environment</p> </li> </ul>"},{"location":"#features","title":"Features","text":"<p>Excerpt of features this cluster provides:</p> <ul> <li>Provision nodes, including k3s, via ansible</li> <li>GitOps based cluster management with ArgoCD</li> <li>Encrypted secrets with sops</li> <li>Every exposed service uses SSO with Authelia</li> <li>File backups from persistant volumes<ul> <li>Backup any folder to a restic supported storage backend</li> <li>Delete old backups (Daily, Weekly, Monthly, Always Keep Last)</li> <li>ntfy.sh notification on failure</li> <li>prometheus pushgateway metrics</li> </ul> </li> <li>KubeDoom: Killing whoami containers with a shotgun</li> <li>High Avaliability ControlPlane and LoadBalancer via KubeVIP</li> <li>Monitoring via kube-prometheus-stack</li> <li>Logging via loki</li> <li>Alerting via alertmanager to a selfhosted ntfy</li> <li>Storage managed via longhorn</li> <li>Vagrant based virtual test environment</li> </ul>"},{"location":"#todos","title":"ToDos","text":"<ul> <li>\u2705 Backup<ul> <li>\u2705 Notification on failure</li> </ul> </li> <li>\u2705 Encryption of secrets<ul> <li>\u2705 Rework documentation</li> </ul> </li> <li> Extend Monitoring beyond kube-prometheus-stack defaults</li> <li>\u2705 Migrate renovate to github actions</li> </ul>"},{"location":"argocd/","title":"ArgoCD","text":""},{"location":"argocd/#dependencies","title":"Dependencies","text":"<p>ArgoCD does not support \"real\" dependencies. Therefore, I use the ArgoCD feature sync waves. Sync waves determine the order in which ArgoCD apps are installed.</p> <p>Example: In my case argo-cd requires a Configuration Management Plugin to be installed first. Without the Configurationmanagementplugin, the argo-cd repo-server container won't start.</p>"},{"location":"argocd/#getting-argocd-admin-password","title":"Getting ArgoCD admin password","text":"<p>This shouldn't be necessary anymore because the helm chart supports setting the password value. But I will leave it here for reference.</p> <pre><code>kubectl patch secret -n argocd argocd-secret -p '{\"stringData\": { \"admin.password\": \"'$(htpasswd -bnBC 10 \"\" ${ADMINPASSWORD} | tr -d ':\\n')'\"}}'\n</code></pre>"},{"location":"argocd/#configmanagementplugin","title":"ConfigManagementPlugin","text":"<p>ArgoCD supports ConfigManagementPlugins (CMP). These plugins allow you to extend the functionality of ArgoCD, e.g. by adding a new config management tool. In my case I use a CMP to encrypt helm values before kustomize is applied. See Secret Management for more information.</p>"},{"location":"argocd/#ignoring-values","title":"Ignoring values","text":"<p>The <code>ignoreDifferences</code> field is a powerful feature in Argo CD Application manifests that allows you to specify certain fields in your Kubernetes resources that should be ignored during ArgoCDs comparison between the live state and desired state defined in the Git repository. I use it to tell ArgoCD to e.g. ignore the number of replicas from a deployment or ignore the caBundle value from kube-prometheus-stack webhooks.</p>"},{"location":"argocd/#detailed-explanation","title":"Detailed Explanation:","text":"<pre><code>  ignoreDifferences:\n    - group: apps\n      kind: Deployment\n      name: emby\n      namespace: media\n      jsonPointers:\n        - /spec/replicas\n</code></pre> <ul> <li> <p>group: Specifies the API group of the resource</p> </li> <li> <p>kind: The type of Kubernetes resource to which this rule applies. Here, it is <code>Deployment</code>.</p> </li> <li> <p>name: The name of the specific resource to which the ignore rule applies, in this case, <code>emby</code>.</p> </li> <li> <p>namespace: The namespace in which the resource resides. This indicates where the <code>Deployment</code> object <code>emby</code> is located.</p> </li> <li> <p>jsonPointers: Lists specific JSON paths within the resource's manifest to ignore during synchronization. JSON Pointers are a way to reference specific parts of a JSON object. In this example, <code>- /spec/replicas</code> tells Argo CD to ignore differences in the <code>replicas</code> field of the <code>Deployment</code> specification.</p> </li> </ul>"},{"location":"argocd/#practical-implications","title":"Practical Implications:","text":"<p>Ignoring differences specified in the <code>jsonPointers</code> allows certain dynamic fields within Kubernetes resources to change (e.g., the number of replicas during autoscaling) without triggering an unwanted sync or causing the application to appear out of sync in Argo CD. This is particularly useful for configurations where you want Argo CD to overlook certain operational details that are managed externally or dynamically, rather than being strictly controlled through GitOps.</p>"},{"location":"auth/","title":"Authentication","text":"<p>Authelia runs stateless and uses the following backends:</p> <ul> <li>Session Provider: Redis</li> <li>Storage Provider: MariaDB</li> <li>Notification Provider: SMTP</li> <li>Authentication Provider LDAP</li> </ul>"},{"location":"auth/#authelia","title":"Authelia","text":"<p>Existing OIDC client secrets converted via</p> <p><pre><code>docker run authelia/authelia:latest authelia crypto hash generate pbkdf2 --variant sha512 --password &lt;client_secret&gt;\n</code></pre> Generate new client secrets via:</p> <pre><code>docker run authelia/authelia:latest authelia crypto hash generate pbkdf2 --variant sha512 --random --random.length 72 --random.charset rfc3986\n</code></pre>"},{"location":"auth/#lldap","title":"LLDAP","text":"<p>For user and groups I use LLDAP. It's an opinionated light ldap implementation including a simple webgui. It is configured to use MariaDB as it's storage backend.</p>"},{"location":"backup/","title":"Backup","text":"<p>For backups I need a solution that backups the files within a persistend volume. I've taken a look at velero and the included backup solution from Longhorn but wasn't able to get what I want. Longhorn seems to be doing block based backups and with Velero I wasn't able to create file backups. Althoug I do not exclude that I've missed something.</p> <p>At the moment I'm evaluating restic as a sidecar container. The sidecar container uses crontab to create backups in regular intervals. The container mounts the crontab config and backup script from configmaps and reads the environment variables from a secret, which configure the backup.</p> <p>Backups with restic provide:</p> <ul> <li>\u2705 ... encryption</li> <li>\u2705 ... compression</li> <li>\u2705 ... deduplication</li> <li>\u2705 ... retention policies</li> </ul> <p>So I can upload my backup to any storage backend without leaking any data. And because of deduplication and compression a lot of storage space can be saved. Included retention policies make it easy to delete old backups.</p> <p>Script Features:</p> <ul> <li>Backup any folder to a restic supported storage backend</li> <li>Delete old backups (Daily, Weekly, Monthly, Always Keep Last)</li> <li>ntfy.sh notification on failure</li> <li>prometheus pushgateway metrics</li> </ul> <p>Info</p> <p>The script runs restic and uploads the backup to the storage backend. Additionally it deletes old backups. It get's the configuration from environment variables.</p> Sidecar deployment example<pre><code>apiVersion: apps/v1\nkind: Deployment\nspec:\n  template:\n    spec:\n      containers:\n        - name: backup-sidecar\n          image: ghcr.io/restic/restic:0.17.1\n          envFrom:\n            - secretRef:\n                name: backup-env-configuration\n          imagePullPolicy: IfNotPresent\n          command: [\"/usr/sbin/crond\", \"-f\", \"-d\", \"6\"]\n          volumeMounts:\n            - mountPath: /backup/config\n              name: config\n              readOnly: true\n            - name: crontab-config\n              mountPath: /etc/crontabs/root\n              subPath: crontab\n            - name: backup-script\n              mountPath: /usr/local/bin/backup.sh\n              subPath: backup.sh\n      volumes:\n        - name: crontab-config\n          configMap:\n            name: crontab\n        - name: backup-script\n          configMap:\n            name: crontab-backup-script\n</code></pre> crontab configuration<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: crontab\ndata:\n  crontab: |\n    # m h  dom mon dow   command\n    # Run script every hour\n    0 * * * * /bin/sh /usr/local/bin/backup.sh\n</code></pre> restic configuration<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: backup-env-configuration\nstringData:\n  # Source directory to backup\n  RESTIC_SOURCE: /backup/config\n  # Backup repository (destination)\n  RESTIC_REPOSITORY: s3:s3.eu-central-2.wasabisys.com/path/to/backup\n  # Backup password\n  RESTIC_PASSWORD: &lt;BackupPassword&gt;\n  # Retention policies\n  KEEP_HOURLY: \"48\"\n  KEEP_DAILY: \"7\"\n  KEEP_WEEKLY: \"4\"\n  KEEP_LAST: \"1\"\n  # AWS credentials\n  AWS_ACCESS_KEY_ID: &lt;SecureSuperSecretKey&gt;\n  AWS_SECRET_ACCESS_KEY: &lt;VerySecureSuperSecretKey&gt;\n</code></pre> <p>I won't include the backup script here, because it can change over time. The script is available in my git repository as a ConfigMap. The ConfigMap get's replicated via reflector to all namespaces so all deployments can use the same script. But after changes to the script the pods using the script need to be restarted. This can be done by deleting the pods or by rolling out a new deployment.</p>"},{"location":"backup/#notifications","title":"Notifications","text":"<p>To get notified if a backup fails I'm using ntfy. Ntfy is a simple notification service that can be self-hosted.</p> <p></p>"},{"location":"backup/#pushgateway-integration","title":"Pushgateway Integration","text":"<p>Additionaly the backup script supports integration with a Prometheus Pushgateway to send custom metrics about the backup process. This enables tracking of backup duration, start time, and status.</p>"},{"location":"backup/#configuration","title":"Configuration","text":"<p>To enable metrics pushing to the Pushgateway, the following environment variables should be configured:</p> <ul> <li><code>PUSHGATEWAY_ENABLED</code>: Set this to <code>\"true\"</code> to enable sending metrics to the Pushgateway</li> <li><code>PUSHGATEWAY_URL</code>: Specify the URL of the Pushgateway server where metrics should be sent to</li> </ul>"},{"location":"backup/#metrics-published","title":"Metrics Published","text":"<p>Warning</p> <p>The metrics might change in the future. Currently I'm not realy satisfied. But maybe that's because I wasn't able to create a good grafana dashboard with them yet. I would appreciate any help. See issue #3</p> <p>The script publishes the following metrics to the Pushgateway:</p> <ul> <li><code>backup_duration_seconds</code>: The time, in seconds, that the backup process took</li> <li><code>backup_start_timestamp</code>: The timestamp in epoch at which the backup process began</li> <li><code>backup_status</code>: The status of the backup process, with either <code>status=\"success\"</code> or <code>status=\"failure\"</code></li> </ul> <p>Example Environment Configuration:</p> <pre><code>PUSHGATEWAY_ENABLED: \"true\"\nPUSHGATEWAY_URL: http://pushgateway.monitoring.svc.cluster.local\n</code></pre>"},{"location":"backup/#environment-variables","title":"\ud83d\udcdd Environment Variables","text":"<p>The following environment variables are used to configure the backup script.</p> Environment Variable Default Description <code>RESTIC_SOURCE</code> Unset Source directory to back up using Restic <code>RESTIC_REPOSITORY</code> Unset Destination repository for the backup <code>RESTIC_PASSWORD</code> Unset Password for encrypting the backup <code>RESTIC_HOSTNAME</code> <code>$(hostname | cut -d '-' -f1)</code> Optional. Hostname to use for the backup. Defaults to the pod name. Especially usefull for pods with host networking. <code>AWS_ACCESS_KEY_ID</code> Unset Access key ID for authenticating with an S3 compatible storage backend <code>AWS_SECRET_ACCESS_KEY</code> Unset Secret access key for authenticating with an S3 compatible storage backend <code>RESTIC_RETENTION_POLICIES_ENABLED</code> <code>true</code> Optional. Enable or disable retention policies <code>KEEP_HOURLY</code> 24 Optional. Number of hourly backups to retain <code>KEEP_DAILY</code> 7 Optional. Number of daily backups to keep <code>KEEP_WEEKLY</code> 4 Optional. Number of weekly backups to maintain <code>KEEP_MONTHLY</code> 12 Optional. Number of monthly backups to keep. Not implemented yet. <code>KEEP_YEARLY</code> 0 Optional. Number of yearly backups to keep. Not implemented yet. <code>KEEP_LAST</code> 1 Optional. Total number of most recent backups to keep, irrespective of time-based intervals <code>NTFY_ENABLED</code> false Optional. Indicates whether notification via ntfy is enabled. Possible values are <code>\"true\"</code> or <code>\"false\"</code> <code>NTFY_TITLE</code> <code>${RESTIC_HOSTNAME - Backup failed}</code> Optional. Title of the ntfy notification message. Can be a string or shell command <code>NTFY_CREDS</code> Unset Optional. Credentials for authenticating with the ntfy notification service. Needs to include the <code>-u</code> option <code>NTFY_PRIO</code> 4 Optional. Priority level for the ntfy notification. Determines the importance of the notification <code>NTFY_TAG</code> bangbang Optional. Tags to categorize the ntfy notification, allowing filtering or grouping of messages <code>NTFY_SERVER</code> ntfy.sh Optional. URL of the ntfy server used for sending notifications <code>NTFY_TOPIC</code> backup Optional. Topic on the ntfy server where the message will be sent to <code>PUSHGATEWAY_ENABLED</code> false Optional. Indicates whether sending metrics to the Pushgateway is enabled. Possible values are <code>\"true\"</code> or <code>\"false\"</code> <code>PUSHGATEWAY_URL</code> Unset Optional. URL of the Pushgateway server for sending metrics Example environment variables<pre><code>RESTIC_SOURCE: /backup/config\nRESTIC_REPOSITORY: s3:s3.eu-central-2.wasabisys.com/k3s-at-home-01/emby\nRESTIC_PASSWORD: bDuSsDS7uWf0OGrK4y5SBvEfIKkIVcK3gGZpxsVx6Ya6PfwkWANDZo8mRaoGnCE6\nKEEP_HOURLY: \"48\"\nKEEP_DAILY: \"7\"\nKEEP_WEEKLY: \"4\"\nKEEP_LAST: \"1\"\nAWS_ACCESS_KEY_ID: v1eoAeRYfHhcRsUsW\nAWS_SECRET_ACCESS_KEY: Hlk6wZiKdrqIafYLOdMbw9Z7WfKK8W6ata\nNTFY_ENABLED: \"true\"\nNTFY_TITLE: $(hostname | cut -d '-' -f1) - Backup failed\n# Needs to be with \"-u\"\nNTFY_CREDS: -u mne-adm:qhCVXJvzkf9SkjgFE9RDhtzycKbszdSnVw7fHFgS3cZCDmZMno25yfVhikrnPidS\nNTFY_PRIO: \"4\"\nNTFY_TAG: bangbang\nNTFY_SERVER: https://ntfy.geekbundle.org\nNTFY_TOPIC: kubernetes-at-home\nPUSHGATEWAY_ENABLED: \"true\"\nPUSHGATEWAY_URL: http://pushgateway.monitoring.svc.cluster.local:9091\n</code></pre>"},{"location":"backup/#rclone","title":"rclone","text":"<p>At some point I was also evaluating rclone as a sidecar container. But it doesn't support de-duplication and I want my storage costs to be as low as possible. For history reasons I keep the script that I've written and mounted in the container.</p> <p>rclone-backup.sh<pre><code>#!/bin/sh\n\n# Create run.log only if it does not exist\nif [ ! -f /data/run.log ]; then\n  touch /data/run.log\nfi\n\n# Echo current date to run.log for logging purposes\necho \"$(date) Backup process started\" &gt;&gt; /data/run.log\n\n# Sync source to destination with a backup directory using 'year-month-day' format\nbackup_dir=\"$BUP_DST/$(hostname | cut -d '-' -f1)-$(date +%Y-%m-%d)\"\nrclone --config /config/rclone/rclone.conf sync \"$BUP_SRC\" \"$BUP_DST/$(hostname | cut -d '-' -f 1)\" -v --backup-dir=\"$backup_dir\"\n\n# Calculate the date for retention time\nRETENTION_DATE=$(date -d '7 days ago' +%Y-%m-%d)\n\n# List directories and delete those older than retention time\nrclone --config /config/rclone/rclone.conf lsf \"$BUP_DST/\" --dirs-only | while read dir; do\n  # Extract the date from the directory name\n  dir_date=$(echo \"$dir\" | awk -F'-' '{print $NF}' | sed 's#/##g')\n  # Compare directory date with RETENTION_DATE\n  if [ \"$dir_date\" \\&lt; \"$RETENTION_DATE\" ]; then\n    echo \"Deleting old backup directory: $BUP_DST/$dir\" &gt;&gt; /data/run.log\n    rclone --config /config/rclone/rclone.conf purge \"$BUP_DST/$dir\"\n  fi\ndone\n\n# Log the completion of the backup process\necho \"$(date) Backup process completed\" &gt;&gt; /data/run.log\n</code></pre> This required the following config map mounted to /config/rclone/rclone.conf</p> rclone configuration<pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: rclone-config\n  namespace: whoami\nstringData:\n  rclone.conf: |\n    [WasabiFrankfurt]\n    type = s3\n    provider = Wasabi\n    env_auth = false\n    access_key_id = &lt;SuperSecretImportantPrivy&gt;\n    secret_access_key = &lt;VeryVerySuperSecretImportantPrivy&gt;\n    region = eu-central-2\n    endpoint = s3.eu-central-2.wasabisys.com\n    acl =\n</code></pre>"},{"location":"certmanager/","title":"Certmanager","text":"<p>Extract a certificate with private key:</p> <pre><code>kubectl get secret -n certmanager wildcard-cloudflare-production-01 -o jsonpath=\"{.data['tls\\.crt']}\" | base64 -d\nkubectl get secret -n certmanager wildcard-cloudflare-production-01 -o jsonpath=\"{.data['tls\\.key']}\" | base64 -d\n</code></pre>"},{"location":"databases/","title":"Datenbanken","text":""},{"location":"databases/#mariadb","title":"MariaDB","text":"<p>Execute these commands in the shell of the MariaDB Container.</p> <p>Logging into the database:</p> <pre><code>mariadb -u root --password=$MARIADB_ROOT_PASSWORD\n</code></pre> <p>Show all databases:</p> <pre><code>SHOW databases;\n</code></pre> <p>Show permiissions for user:</p> <pre><code>SHOW GRANTS FOR 'authelia'@'host';\n</code></pre> <p>Create database:</p> <pre><code>CREATE DATABASE authelia;\n</code></pre> <p>Create user with password:</p> <pre><code>CREATE USER 'authelia'@'%' IDENTIFIED BY '&lt;NotSoSecure&gt;';\nGRANT ALL PRIVILEGES ON authelia.* TO 'authelia'@'%';\nFLUSH PRIVILEGES;\n</code></pre> <p>Create user without password, but only allowed from specific networks:</p> <pre><code>CREATE USER 'authelia'@'10.42.0.0/255.255.0.0';\nGRANT ALL PRIVILEGES ON authelia.* TO 'authelia'@'10.42.0.0/255.255.0.0';\nFLUSH PRIVILEGES;\n</code></pre> <p>Delete user:</p> <pre><code>DROP USER 'authelia'@'%';\n</code></pre>"},{"location":"databases/#backup","title":"Backup","text":"<p>The backup for MariaDB in this setup is handled using <code>mariadb-backup</code> and <code>restic</code>. mariadb-backup creates a physical backup and restic stores the file in a repository, providing compression and deduplication.</p> Backup command<pre><code>mariadb-backup --host mariadb.databases.svc.cluster.local --user=root --password=$MARIADB_ROOT_PASSWORD --backup --target-dir=/backup --stream=xbstream &gt; /backup/mariadb.xb\n</code></pre>"},{"location":"databases/#restore-process","title":"Restore Process","text":"<p>The restoration of backups involves retrieving backup snapshots from restic, deserializing, and preparing for database usage.</p> <ol> <li>List Backups: Get available backups stored in the restic repository</li> </ol> <pre><code>restic snapshots --tag MariaDB\n</code></pre> <ol> <li>Restore Snapshot: Extract the latest snapshot to a target directory</li> </ol> <pre><code>restic restore latest --tag MariaDB --target .\n</code></pre> <ol> <li>Unserialize the Backup: Use <code>mbstream</code> to deserialize the backup file</li> </ol> <pre><code>mkdir mariadb_recovery\nmbstream -x &lt;mariadb.xb\n</code></pre> <ol> <li>Prepare the Recovery: Prepare the backup for use. If possible, use the same mariadb-backup version with which the backup was created</li> </ol> <pre><code>mariadb-backup --prepare --target-dir=./mariadb_recovery\n</code></pre> <p>More information about the process in the following fosdem presentation: mariabackup restic</p>"},{"location":"databases/#kubernetes-cronjob-for-automated-backups","title":"Kubernetes CronJob for Automated Backups","text":"<p>A Kubernetes CronJob is used to automate the MariaDB backups. At first it creates the backup with mariadb-backup and then stores it in a restic repository. To ensure that the processes run sequential and not parallel, the backup creation runs as init container and afterwards restic as normal container. For best compatibility the mariadb-backup command must be the same version as the MariaDB server. So the init Container of the cronjob executes the mariadb-backup binary within the mariadb container. This way I always get the correct mariadb-backup version.</p> <p>Explanation:</p> <ul> <li> <p>Service Account and Role: A service account <code>mariadb-backup-serviceaccount</code> is used, bound with a role <code>mariadb-backup-role</code> that has necessary permissions to get the pod name and exec into the container</p> </li> <li> <p>CronJob Configuration: The CronJob is set to run every hour, except 2 o'clock in the night <code>\"0 0-1,3-23 * * *\"</code></p> </li> <li> <p>Backup Initialization: <code>mariadb-backup</code> runs as an <code>initContainer</code> to ensure backups are taken before any other process begins</p> </li> <li> <p>Restic: <code>restic</code> stores the backup in a restic repository which is accessible via an s3 bucket</p> </li> <li> <p>Storage: A <code>PersistentVolumeClaim</code> (<code>longhorn-pvc-mariadb-backupvolume</code>) is used to store the current backup temporarily</p> </li> </ul> <p>The backup strategy is designed to ensure that <code>mariadb-backup</code> uses a version compatible with the running MariaDB server by executing it within the MariaDB container.</p> Shortened Kubernetes CronJob<pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: mariadb-backup\n  namespace: databases\nspec:\n  schedule: \"0 0-1,3-23 * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          serviceAccountName: mariadb-backup-serviceaccount\n          initContainers:\n            - name: kubectl-mariadb-backup\n              image: bitnami/kubectl:latest\n              command:\n                - /bin/sh\n                - -c\n                - &gt;\n                  POD_NAME=$(kubectl get pods -n databases -l app.kubernetes.io/name=mariadb -o jsonpath=\"{.items[0].metadata.name}\") &amp;&amp;\n                  kubectl exec -n databases $POD_NAME -- mariadb-backup --host 127.0.0.1 --user=root --password=$MARIADB_ROOT_PASSWORD --backup --stream=xbstream &gt; /backup/mariadb.xb\n          containers:\n            - name: restic\n              image: ghcr.io/restic/restic:0.17.3\n              command:\n                - /bin/sh\n                - /usr/local/bin/backup.sh\n              volumeMounts:\n                - name: backup\n                  mountPath: /backup\n</code></pre>"},{"location":"databases/#redis","title":"Redis","text":"Database Application 1 authelia 2 paperless <p>Get all keys from a database:</p> <pre><code>redis-cli -n 1 KEYS '*'\n</code></pre> <pre><code>redis-cli SET server:name \"redis server\"\nredis-cli GET server:name\n</code></pre>"},{"location":"helm/","title":"Helm","text":"<p>Show possible helm chart settings:</p> <pre><code>helm show values prometheus-community/kube-prometheus-stack | less\n</code></pre> <p>Show installed settings:</p> <pre><code>helm list --all-namespaces\nhelm get values kube-prometheus-stack -n k3s-monitoring | less\n</code></pre> <p>Show all charts of a repository:</p> <pre><code>helm search repo prometheus-community\n</code></pre> <p>Show all repositories:</p> <pre><code>helm repo list\n</code></pre>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#requirements","title":"Requirements","text":"<ul> <li>git</li> <li>ansible</li> <li>kubectl</li> <li>helm</li> </ul>"},{"location":"install/#preparation","title":"Preparation","text":"<ul> <li>Clone this repository</li> </ul> <pre><code>git clone https://github.com/madic-creates/k3s-git-ops\n</code></pre> <ul> <li>Configure pre-commit-hooks</li> <li>Adapt ansible vars to your needs. For this, copy the \"vagrant.yaml\" to e.g. \"own.yaml\" and use that as ansible extra-vars file.</li> </ul> <pre><code>cd ansible\ncp vars/vagrant.yaml vars/own.yaml\n</code></pre> <p>A short explanation of the vars can be found in the vars file. Because they tend to change, I wont document them here.</p>"},{"location":"install/#ansible-inventory","title":"Ansible inventory","text":"<p>This playbook requires the following host groups:</p> <ul> <li>k3s_server</li> <li>k3s_agent</li> </ul> <p>Example:</p> <pre><code>[k3s_server]\nk3svm1\n\n[k3s_agent]\nk3svm2\nk3svm3\n</code></pre>"},{"location":"install/#installation_1","title":"Installation","text":"<p>Run ansible from within the ansible folder</p> <pre><code>cd ansible\nansible-playbook install.yaml -i inventory --extra-vars \"@vars/own.yaml\" --diff\n</code></pre>"},{"location":"install/#removal","title":"Removal","text":"<pre><code>cd ansible\nansible-playbook remove.yaml -i inventory -l node01.example.com -K --diff\n</code></pre>"},{"location":"install/#argocd","title":"ArgoCD","text":"<pre><code>kubectl kustomize apps/argo-cd --enable-helm | kubectl apply -f -\nkubectl kustomize apps/overlays/vagrant | kubectl apply -f -\n</code></pre> <p>Get the ArgoCD Login password for the admin user:</p> <pre><code>kubectl get secret argocd-initial-admin-secret -n argocd -o jsonpath=\"{.data.password}\" | base64 -d\n</code></pre> <p>Remove ArgoCD</p> <pre><code>kubectl kustomize bootstrap --enable-helm | kubectl delete -f -\n</code></pre>"},{"location":"kubedoom/","title":"Kube DOOM","text":"<p>In the cluster is Kube DOOM installed for the namespace whoami. Set up port forward and access it via VNC (e.g. tigervnc) on 127.0.0.1:5900.</p> <p>VNC Passwort: idbehold</p> Cheat Effect IDDQD Unverwundbar IDKFA Alle Waffen IDSPISPOPD NoClip"},{"location":"loadbalancer/","title":"LoadBalancer","text":"<ul> <li>Ingress Traefik</li> <li>KubeVIP</li> <li>Certmanager</li> </ul>"},{"location":"loadbalancer/#kubevip","title":"KubeVIP","text":"<p>KubeVIP is used in two variants:</p> <ul> <li>HA for Kubernetes API</li> <li>LoadBalancer Provider (cloud-controller)</li> </ul>"},{"location":"loadbalancer/#ha-for-kubernetes-api","title":"HA for Kubernetes API","text":"<p>A kube-vip pod is launched using a DaemonSet to manage a virtual IP (VIP) via ARP. More information: https://kube-vip.io/docs/installation/daemonset/</p> <p>Github Releases: https://github.com/kube-vip/kube-vip/releases</p>"},{"location":"loadbalancer/#updating-kubevip","title":"Updating KubeVIP","text":"<p>Updating the manifests for the ArgoCD app in apps/kubevip-ha</p> <p>Docker</p> <pre><code>export KVVERSION=v0.6.4\ndocker image pull ghcr.io/kube-vip/kube-vip:$KVVERSION\ndocker run --rm ghcr.io/kube-vip/kube-vip:$KVVERSION\n  manifest \\\n  daemonset \\\n  --interface eth1 \\\n  --address 192.168.33.9 \\\n  --inCluster \\\n  --taint \\\n  --controlplane \\\n  --services \\\n  --arp \\\n  --leaderElection\n</code></pre> <p>containerd:</p> <pre><code>export KVVERSION=v0.6.4\n/usr/local/bin/ctr image pull ghcr.io/kube-vip/kube-vip:$KVVERSION\n/usr/local/bin/ctr run --rm --net-host ghcr.io/kube-vip/kube-vip:$KVVERSION vip /kube-vip \\\n  manifest \\\n  daemonset \\\n  --interface eth1 \\\n  --address 192.168.1.230 \\\n  --inCluster \\\n  --taint \\\n  --controlplane \\\n  --services \\\n  --arp \\\n  --leaderElection \\\n  --enableNodeLabeling\n</code></pre> <p>Copy the manifest created to apps/kubevi-ha/kubevip.yaml and remove the creationTimestamps and the status.</p>"},{"location":"loadbalancer/#kubevip_1","title":"KubeVIP","text":"<p>KubeVIP is installed via Ansible because a Kubernetes manifest needs to be created during installation, which kustomize cannot do. Any further configuration is then handled by kustomize.</p>"},{"location":"mkdocs/","title":"mkdocs","text":"<p>This repository builds it's documentation with mkdocs.</p>"},{"location":"mkdocs/#installation","title":"Installation","text":"<pre><code>python3 -m venv --copies mkdocs\nsource mkdocs/bin/activate\n# Update all python packages within the python environment\npython3 -m pip list --outdated --format=json | jq -r '.[] | \"\\(.name)==\\(.latest_version)\"' | xargs --no-run-if-empty -n1 python3 -m pip install -U\n# Install mkdocs requirements\npip install -r requirements.txt\n</code></pre>"},{"location":"mkdocs/#building","title":"Building","text":"<pre><code>mkdocs build --strict --verbose\nmkdocs serve -a 127.0.0.1:8000\n</code></pre>"},{"location":"monitoring/","title":"Monitoring","text":""},{"location":"monitoring/#metrics","title":"Metrics","text":"<p>The kube-prometheus-stack helm chart pre-configures the following components:</p> <ul> <li>Prometheus (Operator)</li> <li>Grafana</li> <li>Node Exporter</li> <li>Alertmanager</li> <li>kube-state-metrics</li> </ul> <p></p> <p>Additionally the Alertmanager sends alerts via webhook to the ntfy-alertmanager which forwards it to a selfhosted ntfy.sh instance.</p> <p></p> <p>The encrypted config-file from the ntfy-alertmanager is basically this.</p> <p>More information can be found in the ntfy-alertmanager config documentation.</p> <pre><code>base-url https://ntfy-alertmanager.k8s.example.com\nhttp-address :8080\nlog-level info\nlog-format text\nalert-mode single\nuser &lt;SuperSecureUser&gt;\npassword &lt;SuperSecurePassword&gt;\nlabels {\n    order \"severity,instance\"\n    severity \"critical\" {\n        priority 5\n        tags \"rotating_light\"\n        icon \"\"\n    }\n    severity \"info\" {\n        priority 1\n    }\n}\nresolved {\n    tags \"resolved,partying_face\"\n    priority 1\n}\nntfy {\n    topic https://ntfy.example.com/kubernetes-at-home\n    access-token &lt;SuperSecureAccessToken&gt;\n}\nalertmanager {\n    silence-duration 24h\n    url http://kube-prometheus-stack-alertmanager.monitoring.svc.cluster.local:9093\n}\ncache {\n    type disabled\n    duration 24h\n    cleanup-interval 1h\n    redis-url redis://redis-master.databases.svc.cluster.local:6379/3\n}\n</code></pre>"},{"location":"monitoring/#logging","title":"Logging","text":"<p>Loki and promtail are used as pod log collector. The grafan helm repository provides multiple loki charts. <code>loki-stack</code> is outdated and shouldn't be used. The other interesting charts are grafana/loki (SSD Mode) and grafan/loki-distributed (microservice mode). More information on the modes (SSD vs. microservices): https://grafana.com/docs/loki/latest/get-started/deployment-modes/. This repository uses the SSD mode.</p> <p></p>"},{"location":"monitoring/#kubernetes-events","title":"Kubernetes Events","text":""},{"location":"pre-commit-hooks/","title":"pre-commit","text":"<p>This repository uses pre-commit to manage pre-commit hooks.</p>"},{"location":"pre-commit-hooks/#installation","title":"Installation","text":"<p>Installing the binary is covered on the homepage of pre-commit.</p> <p>To activate it for this repository:</p> <pre><code>pre-commit install\n</code></pre> <p>pre-commits are managed in the <code>.pre-commit-config.yaml</code> file.</p> <p>Run a test over all files:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"reflector/","title":"Reflector","text":"<p>Reflector  syncs Secrets and ConfigMaps between namespaces. I use it to sync the following objects between namespaces:</p> <ul> <li>Wildcard certificate</li> <li>Backup script</li> </ul>"},{"location":"secretmanagement/","title":"Secret Management","text":"<p>In this repository I use two ways to encrypt secrets, both utilizing sops and age.</p> <ul> <li>Kubernetes Secrets / Manifests are encrypted via KSOPS (described in this document)</li> <li>Kustomize managed Helm values are encrypted via sops and decrypted by an ArgoCD ConfigManagementPlugin</li> </ul> <p>age is the recommended encryption tool for sops as it is more secure and easier to use than gpg.</p>"},{"location":"secretmanagement/#requirements","title":"Requirements","text":"<ul> <li>ksops</li> <li>age</li> </ul>"},{"location":"secretmanagement/#preparation","title":"Preparation","text":"<p>For both variants we need two age keypairs. One for local use and one for ArgoCD.</p> <pre><code># the folder for the age keypairs, consumed by sops\nmkdir -p \"$HOME/.config/sops/age\"\n# local age keypair\nage-keygen -o \"$HOME/.config/sops/age/keys.txt\"\n# argocd age keypair\nage-keygen -o \"$HOME/.config/sops/age/argo-cd.txt\"\n</code></pre> <p>Example output:</p> <pre><code>cat \"$HOME/.config/sops/age/keys.txt\"\n# created: 2024-05-28T07:23:28+02:00\n# public key: age***\nAGE-SECRET-KEY-19***\n</code></pre> <p>ArgoCD needs the private key of the local keypair to decrypt the secrets. So we create a kubernetes secret with the private key that ArgoCD gets mounted.</p> <pre><code>cat \"$HOME/.config/sops/age/argo-cd.txt\" | kubectl create secret generic sops-age --namespace=argocd \\\n--from-file=keys.txt=/dev/stdin\n</code></pre> <p>Adjusted helm values to mount the sops-age secret into the argocd-server pod:</p> values.yaml<pre><code>repoServer:\n  volumes:\n    - name: sops-age\n      secret:\n        secretName: sops-age\n  volumeMounts:\n    - mountPath: /.config/sops/age\n      name: sops-age\n      readOnly: true\n  env:\n    - name: SOPS_AGE_KEY_FILE\n      value: /.config/sops/age/keys.txt\n</code></pre>"},{"location":"secretmanagement/#repository-configuration","title":"Repository configuration","text":"<p>Info</p> <p>The secrets need to be encrypted with both public keys. The ArgoCD key is used to decrypt the secrets in the ArgoCD cluster and the local key is used to de- and encrypt the secrets locally.</p> <p>Create a <code>.sops.yaml</code> file in the repository root. Example:</p> .sops.yaml<pre><code>creation_rules:\n  - path_regex: .*.enc.yaml\n    encrypted_regex: \"^(data|stringData|email|dnsNames|.*(H|h)osts?|hostname|username|password|url|issuer|clientSecret|argocdServerAdminPassword|oidc.config|commonName|literals)$\"\n    age: age1d2g7tgqpfvxulsusn3m608h60h2hne7yqwv5nh5nd24z6h0hgq0skjkhw8,age1q522xtgjrmvr43w7um5rh02ta3yfns635680hz4m7uhw0nfqj5zqgxnz27\n  - path_regex: secrets/argo-cd.age\n    age: age1d2g7tgqpfvxulsusn3m608h60h2hne7yqwv5nh5nd24z6h0hgq0skjkhw8\n</code></pre> <p>The .sops file for this repository differs from this example.</p>"},{"location":"secretmanagement/#kubernetes-secrets-manifests-via-ksops","title":"Kubernetes Secrets / Manifests via KSOPS","text":"<p>To use sops with ArgoCD, you need to mount ksops and the sops-age key into the argocd-server pod. The following helm values start the ksops container as an initContainer, copies the ksops and kustomize binaries into the custom-tools volume and mounts the binaries from the custom-tools volume into the repo server container. The sops-age key is also mounted into the repo server container.</p> values.yaml<pre><code>configs:\n  cm:\n    kustomize.buildOptions: \"--enable-helm --enable-alpha-plugins --enable-exec\"\nrepoServer:\n  # Use init containers to configure custom tooling\n  # https://argoproj.github.io/argo-cd/operator-manual/custom_tools/\n  volumes:\n    - name: custom-tools\n      emptyDir: {}\n  initContainers:\n    - name: install-ksops\n      image: viaductoss/ksops:v4.3.1\n      command: [\"/bin/sh\", \"-c\"]\n      args:\n        - echo \"Installing KSOPS...\";\n          mv ksops /custom-tools/;\n          mv kustomize /custom-tools/;\n          echo \"Done.\";\n      volumeMounts:\n        - mountPath: /custom-tools\n          name: custom-tools\n  volumeMounts:\n    # ksops packages it's own kustomize binary with ksops integration, overrides the argocd kustomize binary\n    - mountPath: /usr/local/bin/kustomize\n      name: custom-tools\n      subPath: kustomize\n    - mountPath: /usr/local/bin/ksops\n      name: custom-tools\n      subPath: ksops\n    - mountPath: /.config/sops/age\n      name: sops-age\n  env:\n    - name: XDG_CONFIG_HOME\n      value: /.config\n</code></pre>"},{"location":"secretmanagement/#adjusting-kustomize-configuration","title":"Adjusting kustomize configuration","text":"<p>Info</p> <p>To tell kustomize to use ksops for decryption, we need to add a generators configuration to the <code>kustomization.yaml</code> file.</p> kustomization.yaml<pre><code>generators:\n  - kustomize-secret-generator.yaml\n</code></pre> kustomize-secret-generator.yaml<pre><code>---\napiVersion: viaduct.ai/v1\nkind: ksops\nmetadata:\n  name: secret-generator\n  annotations:\n    config.kubernetes.io/function: |\n      exec:\n        path: ksops\nfiles:\n  - backup-secrets.enc.yaml\n</code></pre> <p>The backup-secrets.enc.yaml is just a normal kubernetes manifest but with sops encrypted values: Example emby backup-secrets.enc.yaml. Do not add thouse manifests to the resource list of the kustomization.yaml because this would result in a duplicate resource error.</p>"},{"location":"secretmanagement/#kustomize-managed-helm-values","title":"Kustomize managed Helm values","text":"<p>This repository makes heavy use of kustomize rendering helm charts. Kustomize can manage helm values either directly in the kustomization.yaml or in a separate file. The helm values can contain sensitive values and it's not possible to encrypt values in the kustomization.yaml file directly so we need to use a separate helm values file.</p> <p>Info</p> <p>The workflow basically is to create an encrypted values.enc.yaml, tell kustomize to get the helm values from values.yaml and use an ArgoCD ConfigManagementPlugin to decrypt the values.enc.yaml to values.yaml. The ConfigManagementPlugin gets executed when ArgoCD finds a values.enc.yaml before kustomize renders the kubernetes manifests.</p>"},{"location":"secretmanagement/#configuration-of-the-configmanagementplugin","title":"Configuration of the ConfigManagementPlugin","text":"<p>If ArgoCD finds a values.enc.yaml in an application directory, argo-cd runs the CMP cmp-sops-decrypt, which decryptes the file to values.yaml, and then runs kustomize.</p> <p>The ConfigManagementPlugin is configured as a configmap within a separate argo-cd app deployment. Additionaly it needs helm and sops binaries for the argo-cd repo-server which get configured via a sidecar container. So the deployment of it needs to be extended.</p> <p>ConfigManagementPlugin:</p> argocd-cmp-sops-plugin.yaml<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cmp-sops-plugin\n  namespace: argocd\ndata:\n  plugin.yaml: |\n    ---\n    apiVersion: argoproj.io/v1alpha1\n    kind: ConfigManagementPlugin\n    metadata:\n      name: cmp-sops-decrypt\n    spec:\n      version: v1.0\n      generate:\n        command: [sh, -c]\n        args:\n          - sops --decrypt --input-type yaml --output-type yaml values.enc.yaml &gt; values.yaml;\n            kustomize build --enable-helm --enable-alpha-plugins --enable-exec .\n      discover:\n        fileName: \"values.enc.yaml\"\n</code></pre> <p>The adjusted helm values for the argo-cd repo-server. Some of the adjustments are dependent on changes from the previous section (like ksops and kustomize usage).</p> values.yaml<pre><code>repoServer:\n  # Addingcustom tools volume and ConfigManagementPlugin to the repo-server pod deployment\n  volumes:\n    - name: custom-tools\n      emptyDir: {}\n    - name: cmp-tmp\n      emptyDir: {}\n    - name: cmp-sops-plugin\n      configMap:\n        name: argocd-cmp-sops-plugin\n  # Installation of binaries\n  initContainers:\n    - name: install-sops\n      image: ghcr.io/getsops/sops:v3.8.1-alpine\n      command:\n        - /bin/sh\n        - -c\n      args:\n        - echo \"Installing SOPS...\";\n          cp /usr/local/bin/sops /custom-tools/;\n          echo \"Done.\";\n      volumeMounts:\n        - mountPath: /custom-tools\n          name: custom-tools\n    - name: install-helm\n      image: alpine/helm:3.15.1\n      command:\n        - /bin/sh\n        - -c\n      args:\n        - echo \"Installing helm...\"; cp /usr/bin/helm /custom-tools/; echo \"Done.\";\n      volumeMounts:\n        - mountPath: /custom-tools\n          name: custom-tools\n  # Adding Container responsible for the configured ConfigManagementPlugin\n  extraContainers:\n    - name: cmp-sops-plugin\n      command:\n        - \"/var/run/argocd/argocd-cmp-server\"\n      image: alpine:3.20.0\n      imagePullPolicy: IfNotPresent\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 999\n      volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /home/argocd/cmp-server/config/plugin.yaml\n          subPath: plugin.yaml\n          name: cmp-sops-plugin\n        - mountPath: /tmp\n          name: cmp-tmp\n        - mountPath: /usr/local/bin/kustomize\n          name: custom-tools\n          subPath: kustomize\n        - mountPath: /usr/local/bin/ksops\n          name: custom-tools\n          subPath: ksops\n        - mountPath: /usr/local/bin/sops\n          name: custom-tools\n          subPath: sops\n        - mountPath: /usr/local/bin/helm\n          name: custom-tools\n          subPath: helm\n        - mountPath: /.config/sops/age\n          name: sops-age\n          readOnly: true\n  volumeMounts:\n    - mountPath: /usr/local/bin/kustomize\n      name: custom-tools\n      subPath: kustomize\n    - mountPath: /usr/local/bin/ksops\n      name: custom-tools\n      subPath: ksops\n    - mountPath: /.config/sops/age\n      name: sops-age\n      readOnly: true\n</code></pre> <p>This basically builds the plugin container with all required tools on-demand.</p> <p>Of course, the configuration could be way shorter if a container, that already includes the following binaries, would be used as extraContainer \ud83e\udd37</p> <ul> <li>kustomize (from ksops)</li> <li>ksops</li> <li>sops</li> <li>helm</li> </ul> <p>More information about my journey to en- and decrypt values.yaml can be found in the following ksops issue on github: Support kustomize helmCharts valuesFile.</p>"},{"location":"secretmanagement/#en-and-decrypting-helm-values-and-manifests","title":"En- and decrypting helm values and manifests","text":"<p>To encrypt a file inplace, use the following command:</p> <pre><code>sops -e -i secret.enc.yaml\n</code></pre> <p>To decrypt a file inplace, use the following command:</p> <pre><code>sops -d -i secret.enc.yaml\n</code></pre> <p>Tip</p> <p>If you're working with VSCode I can recommend the extension @signageos/vscode-sops which automatically decrypts and encrypts secrets on save.</p> <p>It can also automatically encrypt files which are not yet encrypted. To enable this feature, add the following to your <code>settings.json</code>:</p> <pre><code>{\n  \"sops.creationEnabled\": true\n}\n</code></pre> <p>This will automatically encrypt files which match the <code>creation_rules</code> in the <code>.sops.yaml</code> file.</p>"},{"location":"secrets/","title":"Kubernetes Secrets erstellen","text":""},{"location":"secrets/#via-kubernetes-manifest","title":"Via Kubernetes Manifest","text":"<p>Base64 encoded:</p> <pre><code>echo -n '1f2d1e2e67df' | base64\nMWYyZDFlMmU2N2Rm\n</code></pre> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\ntype: Opaque\ndata:\n  password: MWYyZDFlMmU2N2Rm\n</code></pre> <p>Plain Text</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\ntype: Opaque\nstringData:\n  password: 1f2d1e2e67df\n</code></pre>"},{"location":"secrets/#links","title":"Links","text":"<ul> <li>https://www.mirantis.com/cloud-native-concepts/getting-started-with-kubernetes/what-are-kubernetes-secrets/</li> </ul>"},{"location":"storage/","title":"Distributed Storage","text":""},{"location":"storage/#longhorn","title":"Longhorn","text":"<p>Longhorn is used as primary local distributed storage.</p>"},{"location":"storage/#troubleshooting","title":"Troubleshooting","text":"<p>If a pod can't start and in the events is something like \"Volume is already exclusively attached to one node and can't be attached to anothe\" you need to wait at least 2 hours. Haven't found a solution yet.</p>"},{"location":"testenv/","title":"Test environment","text":"<p>The test environment is based upon virtual machines and created with Vagrant and libvirt. You need a quiet powerful machine to run the full test environment. It can creat up to three systems with the following configuration:</p> <ul> <li>k3svm1: The server node of the k3s cluster<ul> <li>4096MB RAM</li> <li>4 CPU threads</li> </ul> </li> <li>k3svm2: The first worker node of the k3s cluster<ul> <li>2048MB RAM</li> <li>2 CPU threads</li> </ul> </li> <li>k3svm3: The second worker node of the k3s cluster<ul> <li>2048MB RAM</li> <li>2 CPU threads</li> </ul> </li> </ul> <p>I decided to base the test environemnt on virtual machines because I also test different network plugins. This is much easier to do with virtual machines than with containers. Another requirement is ansible, which I use to initially install k3s. This also needs to be tested. During deployment Vagrant uses the same ansible playbook to provision the system(s) as the one used for the production environment. Just the vars are a bit different and can be adjusted in the ansible/vars/vagrant.yaml file.</p> <p>In the subfolder ./shared/${HOSTNAME}/ are files for the configuration of the k3s cluster, like the kubeconfig.</p> <p>The folder ./shared gets also mounted to the virtual machines. This is the place where you can put files you want to share with the virtual machines. Though this requires memory_backing_dir = \"/dev/shm\" in /etc/libvirt/qemu.conf.</p>"},{"location":"testenv/#setting-up-vagrant","title":"Setting up vagrant","text":"<p>At first install vagrant for your system. This wont be covered here. Please refer to the vagrant documentation.</p> <p>Add the libvirt plugin:</p> <pre><code>vagrant plugin install vagrant-libvirt\n</code></pre> <p>Tip</p> <p>If you have, like me, dependency problems with the libvirt plugin, you can use the vagrant-libvirt docker container. I recommend you to use my extended vagrant-libvirt-ansible container with added ansible. Ansible is required for provisioning and by default not installed in the standard vagrant-libvirt container.</p> <p>Start the complete test environment:</p> <pre><code>vagrant up\n</code></pre> <p>Start only the server node (which is enough for many tests):</p> <pre><code>vagrant up k3svm1\n</code></pre> <p>After the installation is finished, it can still take some time till the k3s service is up and running.</p> <p>The ansible ansible/install.yaml playbook, which vagrant automatically runs, will copy the kubeconfig from the server node to shared/k3svm1/k3s.yaml. You can use this file to access the test k3s cluster.</p> <pre><code>export KUBECONFIG=\"$PWD/shared/k3svm1/k3s.yaml\"\n</code></pre> <p>Start the worker nodes separately if needed. Ansible will automatically join the worker nodes to the k3s cluster.</p> <pre><code>vagrant up k3svm2\nvagrant up k3svm3\n</code></pre> <p>Once your virtual machine is up and running, you can log in to it:</p> <pre><code>vagrant ssh k3svm1\n</code></pre> <p>Destroy the test environment:</p> <pre><code>vagrant destroy -f\n</code></pre>"},{"location":"testenv/#notes","title":"Notes","text":"<p>Sometimes vagrant has conflicts with os packages resulting in message like this:</p> <p>conflicting dependencies date (= 3.2.2) and date (= 3.3.4)</p> <p>Set the following environment variable to ignore gem versions or directly use my vagrant-libvirt-ansible container:</p> <pre><code>export VAGRANT_DISABLE_STRICT_DEPENDENCY_ENFORCEMENT=1\n</code></pre>"},{"location":"updates/","title":"Update management","text":"<p>The cluster uses two components to manage updates:</p> <ul> <li>Renovate</li> <li>System Upgrade Controller</li> </ul>"},{"location":"updates/#renovate","title":"Renovate","text":"<p>Renovate is configured as a scheduled github workflow. Additionally, the action can be run manually with optional increased debug log or in dry-run mode. As RENOVATE_TOKEN a github personal access token is used.</p> <p>More information about the required token permissions can be found in the official docs: https://docs.renovatebot.com/modules/platform/github/. I keep it here because I had a hard time finding thoses information \ud83d\ude48</p>"},{"location":"updates/#rules","title":"Rules","text":"<p>Multiple configurations in the renovate.json support renovate in finding updates.</p> <p>Kubernetes Manifests</p> <p>Renovate identifies Kubernetes manifests depending on the file name. The current regex is <code>apps\\/.+\\/k8s\\\\..*\\\\.yaml$</code>. Basically beneath the apps folder it matches every yaml file that begins with k8s.</p> <p>Danger</p> <p>Renovate must not change Kubernetes manifests which contain sops encrypted entries because that would break the file signature. Encrypted Kubernetes manifests aren't allowed to begin with k8s!</p> <p>Auto Updates</p> <p>Renovate will auto-update every entry after 5 days of release. If not explicitly excluded, every minor and patch release gets automatically updated / merged. Major releases need to be approved via Pull Request.</p> <p>Longhorn is an exception. It will never automerge and will only create Pull Requests for releases that are available for 14 days, even for patch releases. Because Longhorn is the storage provider for the cluster, I want it to be a tested release.</p> <p>Image Tags</p> <p>Beside updating image tags in kubernetes manifests, renovate updates image tags in kustomization.yaml files when it finds the keyword <code>image</code> or the following structure:</p> <pre><code>repository: emby/embyserver\ntag: 4.9.0.30\n</code></pre> <p><code>Repository</code> is required for renovate to know, in which repository to search for new versions to update the tag.</p> <p>pre-commit hooks</p> <p>Renovate is able to update pre-commit hooks. But it's still in beta and can lead to problems. Because of this automerge for pre-commit is disabled and every update requires approval through a pull request.</p>"},{"location":"updates/#system-upgrade-controller","title":"System Upgrade Controller","text":"<p>The System Upgrade Controller is a Kubernetes controller that manages the upgrade of a Kubernetes cluster. There are only plans configured to automatically upgrade the cluster to the latest k3s version.</p> <p>The update plans are a separate ArgoCD app because they require CRDs from the system-upgrade-controller. Without those CRDs ArgoCD would not be able to apply the plans and the deployment would fail if they are included in the system-upgrade-controller app.</p>"}]}